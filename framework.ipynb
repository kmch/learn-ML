{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package tensorflow.keras.datasets in tensorflow.keras:\n",
      "\n",
      "NAME\n",
      "    tensorflow.keras.datasets - Small NumPy datasets for debugging/testing.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    boston_housing (package)\n",
      "    cifar10 (package)\n",
      "    cifar100 (package)\n",
      "    fashion_mnist (package)\n",
      "    imdb (package)\n",
      "    mnist (package)\n",
      "    reuters (package)\n",
      "\n",
      "FILE\n",
      "    /home/kmc3817/.conda/envs/keras-cpu/lib/python3.7/site-packages/keras/api/_v2/keras/datasets/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.datasets as datasets\n",
    "help(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(models.Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "# model.add(layers.Dense(16, activation='relu')) #, input_shape=(10000,)))\n",
    "# model.add(layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method compile in module keras.engine.training:\n",
      "\n",
      "compile(optimizer='rmsprop', loss=None, metrics=None, loss_weights=None, weighted_metrics=None, run_eagerly=None, steps_per_execution=None, jit_compile=None, **kwargs) method of keras.engine.sequential.Sequential instance\n",
      "    Configures the model for training.\n",
      "    \n",
      "    Example:\n",
      "    \n",
      "    ```python\n",
      "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
      "                  loss=tf.keras.losses.BinaryCrossentropy(),\n",
      "                  metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
      "                           tf.keras.metrics.FalseNegatives()])\n",
      "    ```\n",
      "    \n",
      "    Args:\n",
      "        optimizer: String (name of optimizer) or optimizer instance. See\n",
      "          `tf.keras.optimizers`.\n",
      "        loss: Loss function. May be a string (name of loss function), or\n",
      "          a `tf.keras.losses.Loss` instance. See `tf.keras.losses`. A loss\n",
      "          function is any callable with the signature `loss = fn(y_true,\n",
      "          y_pred)`, where `y_true` are the ground truth values, and\n",
      "          `y_pred` are the model's predictions.\n",
      "          `y_true` should have shape\n",
      "          `(batch_size, d0, .. dN)` (except in the case of\n",
      "          sparse loss functions such as\n",
      "          sparse categorical crossentropy which expects integer arrays of\n",
      "          shape `(batch_size, d0, .. dN-1)`).\n",
      "          `y_pred` should have shape `(batch_size, d0, .. dN)`.\n",
      "          The loss function should return a float tensor.\n",
      "          If a custom `Loss` instance is\n",
      "          used and reduction is set to `None`, return value has shape\n",
      "          `(batch_size, d0, .. dN-1)` i.e. per-sample or per-timestep loss\n",
      "          values; otherwise, it is a scalar. If the model has multiple\n",
      "          outputs, you can use a different loss on each output by passing a\n",
      "          dictionary or a list of losses. The loss value that will be\n",
      "          minimized by the model will then be the sum of all individual\n",
      "          losses, unless `loss_weights` is specified.\n",
      "        metrics: List of metrics to be evaluated by the model during\n",
      "          training and testing. Each of this can be a string (name of a\n",
      "          built-in function), function or a `tf.keras.metrics.Metric`\n",
      "          instance. See `tf.keras.metrics`. Typically you will use\n",
      "          `metrics=['accuracy']`.\n",
      "          A function is any callable with the signature `result = fn(y_true,\n",
      "          y_pred)`. To specify different metrics for different outputs of a\n",
      "          multi-output model, you could also pass a dictionary, such as\n",
      "          `metrics={'output_a':'accuracy', 'output_b':['accuracy', 'mse']}`.\n",
      "          You can also pass a list to specify a metric or a list of metrics\n",
      "          for each output, such as\n",
      "          `metrics=[['accuracy'], ['accuracy', 'mse']]`\n",
      "          or `metrics=['accuracy', ['accuracy', 'mse']]`. When you pass the\n",
      "          strings 'accuracy' or 'acc', we convert this to one of\n",
      "          `tf.keras.metrics.BinaryAccuracy`,\n",
      "          `tf.keras.metrics.CategoricalAccuracy`,\n",
      "          `tf.keras.metrics.SparseCategoricalAccuracy` based on the loss\n",
      "          function used and the model output shape. We do a similar\n",
      "          conversion for the strings 'crossentropy' and 'ce' as well.\n",
      "          The metrics passed here are evaluated without sample weighting; if\n",
      "          you would like sample weighting to apply, you can specify your\n",
      "          metrics via the `weighted_metrics` argument instead.\n",
      "        loss_weights: Optional list or dictionary specifying scalar\n",
      "          coefficients (Python floats) to weight the loss contributions of\n",
      "          different model outputs. The loss value that will be minimized by\n",
      "          the model will then be the *weighted sum* of all individual\n",
      "          losses, weighted by the `loss_weights` coefficients.  If a list,\n",
      "          it is expected to have a 1:1 mapping to the model's outputs. If a\n",
      "          dict, it is expected to map output names (strings) to scalar\n",
      "          coefficients.\n",
      "        weighted_metrics: List of metrics to be evaluated and weighted by\n",
      "          `sample_weight` or `class_weight` during training and testing.\n",
      "        run_eagerly: Bool. Defaults to `False`. If `True`, this `Model`'s\n",
      "          logic will not be wrapped in a `tf.function`. Recommended to leave\n",
      "          this as `None` unless your `Model` cannot be run inside a\n",
      "          `tf.function`. `run_eagerly=True` is not supported when using\n",
      "          `tf.distribute.experimental.ParameterServerStrategy`.\n",
      "        steps_per_execution: Int. Defaults to 1. The number of batches to\n",
      "          run during each `tf.function` call. Running multiple batches\n",
      "          inside a single `tf.function` call can greatly improve performance\n",
      "          on TPUs or small models with a large Python overhead. At most, one\n",
      "          full epoch will be run each execution. If a number larger than the\n",
      "          size of the epoch is passed, the execution will be truncated to\n",
      "          the size of the epoch. Note that if `steps_per_execution` is set\n",
      "          to `N`, `Callback.on_batch_begin` and `Callback.on_batch_end`\n",
      "          methods will only be called every `N` batches (i.e. before/after\n",
      "          each `tf.function` execution).\n",
      "        jit_compile: If `True`, compile the model training step with XLA.\n",
      "          [XLA](https://www.tensorflow.org/xla) is an optimizing compiler\n",
      "          for machine learning.\n",
      "          `jit_compile` is not enabled for by default.\n",
      "          This option cannot be enabled with `run_eagerly=True`.\n",
      "          Note that `jit_compile=True`\n",
      "          may not necessarily work for all models.\n",
      "          For more information on supported operations please refer to the\n",
      "          [XLA documentation](https://www.tensorflow.org/xla).\n",
      "          Also refer to\n",
      "          [known XLA issues](https://www.tensorflow.org/xla/known_issues)\n",
      "          for more details.\n",
      "        **kwargs: Arguments supported for backwards compatibility only.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.compile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "from keras import losses\n",
    "from keras import metrics\n",
    "# model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001), \n",
    "#               loss=losses.binary_crossentropy,\n",
    "#               metrics=[metrics.binary_accuracy]\n",
    "# or more concisely:\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package keras.optimizers in keras:\n",
      "\n",
      "NAME\n",
      "    keras.optimizers - Built-in optimizer classes.\n",
      "\n",
      "DESCRIPTION\n",
      "    For more examples see the base class `tf.keras.optimizers.Optimizer`.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    legacy (package)\n",
      "    legacy_learning_rate_decay\n",
      "    optimizer_experimental (package)\n",
      "    optimizer_v1\n",
      "    optimizer_v2 (package)\n",
      "    schedules (package)\n",
      "\n",
      "SUBMODULES\n",
      "    adadelta_experimental\n",
      "    adadelta_legacy\n",
      "    adadelta_v2\n",
      "    adagrad_experimental\n",
      "    adagrad_legacy\n",
      "    adagrad_v2\n",
      "    adam_experimental\n",
      "    adam_legacy\n",
      "    adam_v2\n",
      "    adamax_experimental\n",
      "    adamax_legacy\n",
      "    adamax_v2\n",
      "    adamw_experimental\n",
      "    base_optimizer_v2\n",
      "    ftrl\n",
      "    ftrl_experimental\n",
      "    ftrl_legacy\n",
      "    gradient_descent_v2\n",
      "    nadam_experimental\n",
      "    nadam_legacy\n",
      "    nadam_v2\n",
      "    optimizer_legacy\n",
      "    rmsprop_experimental\n",
      "    rmsprop_legacy\n",
      "    rmsprop_v2\n",
      "    sgd_experimental\n",
      "    sgd_legacy\n",
      "\n",
      "FUNCTIONS\n",
      "    deserialize(config, custom_objects=None, **kwargs)\n",
      "        Inverse of the `serialize` function.\n",
      "        \n",
      "        Args:\n",
      "            config: Optimizer configuration dictionary.\n",
      "            custom_objects: Optional dictionary mapping names (strings) to custom\n",
      "              objects (classes and functions) to be considered during\n",
      "              deserialization.\n",
      "        \n",
      "        Returns:\n",
      "            A Keras Optimizer instance.\n",
      "    \n",
      "    get(identifier, **kwargs)\n",
      "        Retrieves a Keras Optimizer instance.\n",
      "        \n",
      "        Args:\n",
      "            identifier: Optimizer identifier, one of\n",
      "                - String: name of an optimizer\n",
      "                - Dictionary: configuration dictionary.\n",
      "                - Keras Optimizer instance (it will be returned unchanged).\n",
      "                - TensorFlow Optimizer instance (it will be wrapped as a Keras\n",
      "                  Optimizer).\n",
      "        \n",
      "        Returns:\n",
      "            A Keras Optimizer instance.\n",
      "        \n",
      "        Raises:\n",
      "            ValueError: If `identifier` cannot be interpreted.\n",
      "    \n",
      "    serialize(optimizer)\n",
      "        Serialize the optimizer configuration to JSON compatible python dict.\n",
      "        \n",
      "        The configuration can be used for persistence and reconstruct the\n",
      "        `Optimizer` instance again.\n",
      "        \n",
      "        >>> tf.keras.optimizers.serialize(tf.keras.optimizers.SGD())\n",
      "        {'class_name': 'SGD', 'config': {'name': 'SGD', 'learning_rate': 0.01,\n",
      "                                         'decay': 0.0, 'momentum': 0.0,\n",
      "                                         'nesterov': False}}\n",
      "        \n",
      "        Args:\n",
      "          optimizer: An `Optimizer` instance to serialize.\n",
      "        \n",
      "        Returns:\n",
      "          Python dict which contains the configuration of the input optimizer.\n",
      "\n",
      "DATA\n",
      "    keras_export = functools.partial(<class 'tensorflow.python.util.tf_exp...\n",
      "\n",
      "FILE\n",
      "    /home/kmc3817/.conda/envs/keras-cpu/lib/python3.7/site-packages/keras/optimizers/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module keras.losses in keras:\n",
      "\n",
      "NAME\n",
      "    keras.losses - Built-in loss functions.\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        Loss\n",
      "            LossFunctionWrapper\n",
      "                BinaryCrossentropy\n",
      "                BinaryFocalCrossentropy\n",
      "                CategoricalCrossentropy\n",
      "                CategoricalHinge\n",
      "                CosineSimilarity\n",
      "                Hinge\n",
      "                Huber\n",
      "                KLDivergence\n",
      "                LogCosh\n",
      "                MeanAbsoluteError\n",
      "                MeanAbsolutePercentageError\n",
      "                MeanSquaredError\n",
      "                MeanSquaredLogarithmicError\n",
      "                Poisson\n",
      "                SparseCategoricalCrossentropy\n",
      "                SquaredHinge\n",
      "    \n",
      "    class BinaryCrossentropy(LossFunctionWrapper)\n",
      "     |  BinaryCrossentropy(from_logits=False, label_smoothing=0.0, axis=-1, reduction='auto', name='binary_crossentropy')\n",
      "     |  \n",
      "     |  Computes the cross-entropy loss between true labels and predicted labels.\n",
      "     |  \n",
      "     |  Use this cross-entropy loss for binary (0 or 1) classification applications.\n",
      "     |  The loss function requires the following inputs:\n",
      "     |  \n",
      "     |  - `y_true` (true label): This is either 0 or 1.\n",
      "     |  - `y_pred` (predicted value): This is the model's prediction, i.e, a single\n",
      "     |    floating-point value which either represents a\n",
      "     |    [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]\n",
      "     |    when `from_logits=True`) or a probability (i.e, value in [0., 1.] when\n",
      "     |    `from_logits=False`).\n",
      "     |  \n",
      "     |  **Recommended Usage:** (set `from_logits=True`)\n",
      "     |  \n",
      "     |  With `tf.keras` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(\n",
      "     |    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
      "     |    ....\n",
      "     |  )\n",
      "     |  ```\n",
      "     |  \n",
      "     |  As a standalone function:\n",
      "     |  \n",
      "     |  >>> # Example 1: (batch_size = 1, number of samples = 4)\n",
      "     |  >>> y_true = [0, 1, 0, 0]\n",
      "     |  >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n",
      "     |  >>> bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
      "     |  >>> bce(y_true, y_pred).numpy()\n",
      "     |  0.865\n",
      "     |  \n",
      "     |  >>> # Example 2: (batch_size = 2, number of samples = 4)\n",
      "     |  >>> y_true = [[0, 1], [0, 0]]\n",
      "     |  >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n",
      "     |  >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
      "     |  >>> bce(y_true, y_pred).numpy()\n",
      "     |  0.865\n",
      "     |  >>> # Using 'sample_weight' attribute\n",
      "     |  >>> bce(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()\n",
      "     |  0.243\n",
      "     |  >>> # Using 'sum' reduction` type.\n",
      "     |  >>> bce = tf.keras.losses.BinaryCrossentropy(from_logits=True,\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> bce(y_true, y_pred).numpy()\n",
      "     |  1.730\n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> bce = tf.keras.losses.BinaryCrossentropy(from_logits=True,\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> bce(y_true, y_pred).numpy()\n",
      "     |  array([0.235, 1.496], dtype=float32)\n",
      "     |  \n",
      "     |  **Default Usage:** (set `from_logits=False`)\n",
      "     |  \n",
      "     |  >>> # Make the following updates to the above \"Recommended Usage\" section\n",
      "     |  >>> # 1. Set `from_logits=False`\n",
      "     |  >>> tf.keras.losses.BinaryCrossentropy() # OR ...('from_logits=False')\n",
      "     |  >>> # 2. Update `y_pred` to use probabilities instead of logits\n",
      "     |  >>> y_pred = [0.6, 0.3, 0.2, 0.8] # OR [[0.6, 0.3], [0.2, 0.8]]\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BinaryCrossentropy\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, from_logits=False, label_smoothing=0.0, axis=-1, reduction='auto', name='binary_crossentropy')\n",
      "     |      Initializes `BinaryCrossentropy` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        from_logits: Whether to interpret `y_pred` as a tensor of\n",
      "     |          [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n",
      "     |          assume that `y_pred` contains probabilities (i.e., values in [0,\n",
      "     |          1]).\n",
      "     |        label_smoothing: Float in [0, 1]. When 0, no smoothing occurs. When >\n",
      "     |          0, we compute the loss between the predicted labels and a smoothed\n",
      "     |          version of the true labels, where the smoothing squeezes the labels\n",
      "     |          towards 0.5.  Larger values of `label_smoothing` correspond to\n",
      "     |          heavier smoothing.\n",
      "     |        axis: The axis along which to compute crossentropy (the features\n",
      "     |          axis).  Defaults to -1.\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Name for the op. Defaults to 'binary_crossentropy'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class BinaryFocalCrossentropy(LossFunctionWrapper)\n",
      "     |  BinaryFocalCrossentropy(apply_class_balancing=False, alpha=0.25, gamma=2.0, from_logits=False, label_smoothing=0.0, axis=-1, reduction='auto', name='binary_focal_crossentropy')\n",
      "     |  \n",
      "     |  Computes the focal cross-entropy loss between true labels and predictions.\n",
      "     |  \n",
      "     |  Binary cross-entropy loss is often used for binary (0 or 1) classification\n",
      "     |  tasks. The loss function requires the following inputs:\n",
      "     |  \n",
      "     |  - `y_true` (true label): This is either 0 or 1.\n",
      "     |  - `y_pred` (predicted value): This is the model's prediction, i.e, a single\n",
      "     |    floating-point value which either represents a\n",
      "     |    [logit](https://en.wikipedia.org/wiki/Logit), (i.e, value in [-inf, inf]\n",
      "     |    when `from_logits=True`) or a probability (i.e, value in `[0., 1.]` when\n",
      "     |    `from_logits=False`).\n",
      "     |  \n",
      "     |  According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n",
      "     |  helps to apply a \"focal factor\" to down-weight easy examples and focus more\n",
      "     |  on hard examples. By default, the focal tensor is computed as follows:\n",
      "     |  \n",
      "     |  `focal_factor = (1 - output) ** gamma` for class 1\n",
      "     |  `focal_factor = output ** gamma` for class 0\n",
      "     |  where `gamma` is a focusing parameter. When `gamma=0`, this function is\n",
      "     |  equivalent to the binary crossentropy loss.\n",
      "     |  \n",
      "     |  With the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(\n",
      "     |    loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),\n",
      "     |    ....\n",
      "     |  )\n",
      "     |  ```\n",
      "     |  \n",
      "     |  As a standalone function:\n",
      "     |  \n",
      "     |  >>> # Example 1: (batch_size = 1, number of samples = 4)\n",
      "     |  >>> y_true = [0, 1, 0, 0]\n",
      "     |  >>> y_pred = [-18.6, 0.51, 2.94, -12.8]\n",
      "     |  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=2,\n",
      "     |  ...                                                from_logits=True)\n",
      "     |  >>> loss(y_true, y_pred).numpy()\n",
      "     |  0.691\n",
      "     |  \n",
      "     |  >>> # Apply class weight\n",
      "     |  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
      "     |  ...     apply_class_balancing=True, gamma=2, from_logits=True)\n",
      "     |  >>> loss(y_true, y_pred).numpy()\n",
      "     |  0.51\n",
      "     |  \n",
      "     |  >>> # Example 2: (batch_size = 2, number of samples = 4)\n",
      "     |  >>> y_true = [[0, 1], [0, 0]]\n",
      "     |  >>> y_pred = [[-18.6, 0.51], [2.94, -12.8]]\n",
      "     |  >>> # Using default 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=3,\n",
      "     |  ...                                                from_logits=True)\n",
      "     |  >>> loss(y_true, y_pred).numpy()\n",
      "     |  0.647\n",
      "     |  \n",
      "     |  >>> # Apply class weight\n",
      "     |  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
      "     |  ...     apply_class_balancing=True, gamma=3, from_logits=True)\n",
      "     |  >>> loss(y_true, y_pred).numpy()\n",
      "     |  0.482\n",
      "     |  \n",
      "     |  >>> # Using 'sample_weight' attribute with focal effect\n",
      "     |  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=3,\n",
      "     |  ...                                                from_logits=True)\n",
      "     |  >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()\n",
      "     |  0.133\n",
      "     |  \n",
      "     |  >>> # Apply class weight\n",
      "     |  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
      "     |  ...     apply_class_balancing=True, gamma=3, from_logits=True)\n",
      "     |  >>> loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()\n",
      "     |  0.097\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction` type.\n",
      "     |  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(gamma=4,\n",
      "     |  ...                                                from_logits=True,\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> loss(y_true, y_pred).numpy()\n",
      "     |  1.222\n",
      "     |  \n",
      "     |  >>> # Apply class weight\n",
      "     |  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
      "     |  ...     apply_class_balancing=True, gamma=4, from_logits=True,\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> loss(y_true, y_pred).numpy()\n",
      "     |  0.914\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
      "     |  ...     gamma=5, from_logits=True,\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> loss(y_true, y_pred).numpy()\n",
      "     |  array([0.0017 1.1561], dtype=float32)\n",
      "     |  \n",
      "     |  >>> # Apply class weight\n",
      "     |  >>> loss = tf.keras.losses.BinaryFocalCrossentropy(\n",
      "     |  ...     apply_class_balancing=True, gamma=5, from_logits=True,\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> loss(y_true, y_pred).numpy()\n",
      "     |  array([0.0004 0.8670], dtype=float32)\n",
      "     |  \n",
      "     |  \n",
      "     |  Args:\n",
      "     |    apply_class_balancing: A bool, whether to apply weight balancing on the\n",
      "     |      binary classes 0 and 1.\n",
      "     |    alpha: A weight balancing factor for class 1, default is `0.25` as\n",
      "     |      mentioned in reference [Lin et al., 2018](\n",
      "     |      https://arxiv.org/pdf/1708.02002.pdf).  The weight for class 0 is\n",
      "     |      `1.0 - alpha`.\n",
      "     |    gamma: A focusing parameter used to compute the focal factor, default is\n",
      "     |      `2.0` as mentioned in the reference\n",
      "     |      [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf).\n",
      "     |    from_logits: Whether to interpret `y_pred` as a tensor of\n",
      "     |      [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n",
      "     |      assume that `y_pred` are probabilities (i.e., values in `[0, 1]`).\n",
      "     |    label_smoothing: Float in `[0, 1]`. When `0`, no smoothing occurs. When >\n",
      "     |      `0`, we compute the loss between the predicted labels and a smoothed\n",
      "     |      version of the true labels, where the smoothing squeezes the labels\n",
      "     |      towards `0.5`. Larger values of `label_smoothing` correspond to heavier\n",
      "     |      smoothing.\n",
      "     |    axis: The axis along which to compute crossentropy (the features axis).\n",
      "     |      Defaults to `-1`.\n",
      "     |    reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |      loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |      option will be determined by the usage context. For almost all cases\n",
      "     |      this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |      `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |      `tf.keras`, `compile()` and `fit()`, using `SUM_OVER_BATCH_SIZE` or\n",
      "     |      `AUTO` will raise an error. Please see this custom training [tutorial](\n",
      "     |      https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |      more details.\n",
      "     |    name: Name for the op. Defaults to 'binary_focal_crossentropy'.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BinaryFocalCrossentropy\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, apply_class_balancing=False, alpha=0.25, gamma=2.0, from_logits=False, label_smoothing=0.0, axis=-1, reduction='auto', name='binary_focal_crossentropy')\n",
      "     |      Initializes `BinaryFocalCrossentropy` instance.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CategoricalCrossentropy(LossFunctionWrapper)\n",
      "     |  CategoricalCrossentropy(from_logits=False, label_smoothing=0.0, axis=-1, reduction='auto', name='categorical_crossentropy')\n",
      "     |  \n",
      "     |  Computes the crossentropy loss between the labels and predictions.\n",
      "     |  \n",
      "     |  Use this crossentropy loss function when there are two or more label\n",
      "     |  classes. We expect labels to be provided in a `one_hot` representation. If\n",
      "     |  you want to provide labels as integers, please use\n",
      "     |  `SparseCategoricalCrossentropy` loss.  There should be `# classes` floating\n",
      "     |  point values per feature.\n",
      "     |  \n",
      "     |  In the snippet below, there is `# classes` floating pointing values per\n",
      "     |  example. The shape of both `y_pred` and `y_true` are\n",
      "     |  `[batch_size, num_classes]`.\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[0, 1, 0], [0, 0, 1]]\n",
      "     |  >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> cce = tf.keras.losses.CategoricalCrossentropy()\n",
      "     |  >>> cce(y_true, y_pred).numpy()\n",
      "     |  1.177\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> cce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()\n",
      "     |  0.814\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> cce = tf.keras.losses.CategoricalCrossentropy(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> cce(y_true, y_pred).numpy()\n",
      "     |  2.354\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> cce = tf.keras.losses.CategoricalCrossentropy(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> cce(y_true, y_pred).numpy()\n",
      "     |  array([0.0513, 2.303], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd',\n",
      "     |                loss=tf.keras.losses.CategoricalCrossentropy())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CategoricalCrossentropy\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, from_logits=False, label_smoothing=0.0, axis=-1, reduction='auto', name='categorical_crossentropy')\n",
      "     |      Initializes `CategoricalCrossentropy` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n",
      "     |          default, we assume that `y_pred` encodes a probability distribution.\n",
      "     |        label_smoothing: Float in [0, 1]. When > 0, label values are smoothed,\n",
      "     |          meaning the confidence on label values are relaxed. For example, if\n",
      "     |          `0.1`, use `0.1 / num_classes` for non-target labels and\n",
      "     |          `0.9 + 0.1 / num_classes` for target labels.\n",
      "     |        axis: The axis along which to compute crossentropy (the features\n",
      "     |          axis). Defaults to -1.\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance.\n",
      "     |          Defaults to 'categorical_crossentropy'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CategoricalHinge(LossFunctionWrapper)\n",
      "     |  CategoricalHinge(reduction='auto', name='categorical_hinge')\n",
      "     |  \n",
      "     |  Computes the categorical hinge loss between `y_true` and `y_pred`.\n",
      "     |  \n",
      "     |  `loss = maximum(neg - pos + 1, 0)`\n",
      "     |  where `neg=maximum((1-y_true)*y_pred) and pos=sum(y_true*y_pred)`\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[0, 1], [0, 0]]\n",
      "     |  >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> h = tf.keras.losses.CategoricalHinge()\n",
      "     |  >>> h(y_true, y_pred).numpy()\n",
      "     |  1.4\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> h(y_true, y_pred, sample_weight=[1, 0]).numpy()\n",
      "     |  0.6\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> h = tf.keras.losses.CategoricalHinge(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> h(y_true, y_pred).numpy()\n",
      "     |  2.8\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> h = tf.keras.losses.CategoricalHinge(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> h(y_true, y_pred).numpy()\n",
      "     |  array([1.2, 1.6], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalHinge())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CategoricalHinge\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reduction='auto', name='categorical_hinge')\n",
      "     |      Initializes `CategoricalHinge` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance. Defaults to 'categorical_hinge'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CosineSimilarity(LossFunctionWrapper)\n",
      "     |  CosineSimilarity(axis=-1, reduction='auto', name='cosine_similarity')\n",
      "     |  \n",
      "     |  Computes the cosine similarity between labels and predictions.\n",
      "     |  \n",
      "     |  Note that it is a number between -1 and 1. When it is a negative number\n",
      "     |  between -1 and 0, 0 indicates orthogonality and values closer to -1\n",
      "     |  indicate greater similarity. The values closer to 1 indicate greater\n",
      "     |  dissimilarity. This makes it usable as a loss function in a setting\n",
      "     |  where you try to maximize the proximity between predictions and targets.\n",
      "     |  If either `y_true` or `y_pred` is a zero vector, cosine similarity will be 0\n",
      "     |  regardless of the proximity between predictions and targets.\n",
      "     |  \n",
      "     |  `loss = -sum(l2_norm(y_true) * l2_norm(y_pred))`\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[0., 1.], [1., 1.]]\n",
      "     |  >>> y_pred = [[1., 0.], [1., 1.]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n",
      "     |  >>> # l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]\n",
      "     |  >>> # l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]\n",
      "     |  >>> # l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]\n",
      "     |  >>> # loss = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))\n",
      "     |  >>> #       = -((0. + 0.) +  (0.5 + 0.5)) / 2\n",
      "     |  >>> cosine_loss(y_true, y_pred).numpy()\n",
      "     |  -0.5\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> cosine_loss(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()\n",
      "     |  -0.0999\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> cosine_loss = tf.keras.losses.CosineSimilarity(axis=1,\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> cosine_loss(y_true, y_pred).numpy()\n",
      "     |  -0.999\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> cosine_loss = tf.keras.losses.CosineSimilarity(axis=1,\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> cosine_loss(y_true, y_pred).numpy()\n",
      "     |  array([-0., -0.999], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd',\n",
      "     |                loss=tf.keras.losses.CosineSimilarity(axis=1))\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Args:\n",
      "     |    axis: The axis along which the cosine similarity is computed\n",
      "     |      (the features axis). Defaults to -1.\n",
      "     |    reduction: Type of `tf.keras.losses.Reduction` to apply to loss.\n",
      "     |      Default value is `AUTO`. `AUTO` indicates that the reduction option will\n",
      "     |      be determined by the usage context. For almost all cases this defaults\n",
      "     |      to `SUM_OVER_BATCH_SIZE`. When used with `tf.distribute.Strategy`,\n",
      "     |      outside of built-in training loops such as `tf.keras` `compile` and\n",
      "     |      `fit`, using `AUTO` or `SUM_OVER_BATCH_SIZE` will raise an error. Please\n",
      "     |      see this custom training [tutorial](\n",
      "     |      https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |      more details.\n",
      "     |    name: Optional name for the instance.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CosineSimilarity\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, axis=-1, reduction='auto', name='cosine_similarity')\n",
      "     |      Initializes `LossFunctionWrapper` class.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        fn: The loss function to wrap, with signature `fn(y_true, y_pred,\n",
      "     |          **kwargs)`.\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance.\n",
      "     |        **kwargs: The keyword arguments that are passed on to `fn`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Hinge(LossFunctionWrapper)\n",
      "     |  Hinge(reduction='auto', name='hinge')\n",
      "     |  \n",
      "     |  Computes the hinge loss between `y_true` and `y_pred`.\n",
      "     |  \n",
      "     |  `loss = maximum(1 - y_true * y_pred, 0)`\n",
      "     |  \n",
      "     |  `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n",
      "     |  provided we will convert them to -1 or 1.\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[0., 1.], [0., 0.]]\n",
      "     |  >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> h = tf.keras.losses.Hinge()\n",
      "     |  >>> h(y_true, y_pred).numpy()\n",
      "     |  1.3\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> h(y_true, y_pred, sample_weight=[1, 0]).numpy()\n",
      "     |  0.55\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> h = tf.keras.losses.Hinge(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> h(y_true, y_pred).numpy()\n",
      "     |  2.6\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> h = tf.keras.losses.Hinge(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> h(y_true, y_pred).numpy()\n",
      "     |  array([1.1, 1.5], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd', loss=tf.keras.losses.Hinge())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Hinge\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reduction='auto', name='hinge')\n",
      "     |      Initializes `Hinge` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance. Defaults to 'hinge'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Huber(LossFunctionWrapper)\n",
      "     |  Huber(delta=1.0, reduction='auto', name='huber_loss')\n",
      "     |  \n",
      "     |  Computes the Huber loss between `y_true` and `y_pred`.\n",
      "     |  \n",
      "     |  For each value x in `error = y_true - y_pred`:\n",
      "     |  \n",
      "     |  ```\n",
      "     |  loss = 0.5 * x^2                  if |x| <= d\n",
      "     |  loss = 0.5 * d^2 + d * (|x| - d)  if |x| > d\n",
      "     |  ```\n",
      "     |  where d is `delta`. See: https://en.wikipedia.org/wiki/Huber_loss\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[0, 1], [0, 0]]\n",
      "     |  >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> h = tf.keras.losses.Huber()\n",
      "     |  >>> h(y_true, y_pred).numpy()\n",
      "     |  0.155\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> h(y_true, y_pred, sample_weight=[1, 0]).numpy()\n",
      "     |  0.09\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> h = tf.keras.losses.Huber(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> h(y_true, y_pred).numpy()\n",
      "     |  0.31\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> h = tf.keras.losses.Huber(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> h(y_true, y_pred).numpy()\n",
      "     |  array([0.18, 0.13], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd', loss=tf.keras.losses.Huber())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Huber\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, delta=1.0, reduction='auto', name='huber_loss')\n",
      "     |      Initializes `Huber` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        delta: A float, the point where the Huber loss function changes from a\n",
      "     |          quadratic to linear.\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance. Defaults to 'huber_loss'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class KLDivergence(LossFunctionWrapper)\n",
      "     |  KLDivergence(reduction='auto', name='kl_divergence')\n",
      "     |  \n",
      "     |  Computes Kullback-Leibler divergence loss between `y_true` and `y_pred`.\n",
      "     |  \n",
      "     |  `loss = y_true * log(y_true / y_pred)`\n",
      "     |  \n",
      "     |  See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[0, 1], [0, 0]]\n",
      "     |  >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> kl = tf.keras.losses.KLDivergence()\n",
      "     |  >>> kl(y_true, y_pred).numpy()\n",
      "     |  0.458\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> kl(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()\n",
      "     |  0.366\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> kl = tf.keras.losses.KLDivergence(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> kl(y_true, y_pred).numpy()\n",
      "     |  0.916\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> kl = tf.keras.losses.KLDivergence(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> kl(y_true, y_pred).numpy()\n",
      "     |  array([0.916, -3.08e-06], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd', loss=tf.keras.losses.KLDivergence())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KLDivergence\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reduction='auto', name='kl_divergence')\n",
      "     |      Initializes `KLDivergence` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance. Defaults to 'kl_divergence'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LogCosh(LossFunctionWrapper)\n",
      "     |  LogCosh(reduction='auto', name='log_cosh')\n",
      "     |  \n",
      "     |  Computes the logarithm of the hyperbolic cosine of the prediction error.\n",
      "     |  \n",
      "     |  `logcosh = log((exp(x) + exp(-x))/2)`,\n",
      "     |  where x is the error `y_pred - y_true`.\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[0., 1.], [0., 0.]]\n",
      "     |  >>> y_pred = [[1., 1.], [0., 0.]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> l = tf.keras.losses.LogCosh()\n",
      "     |  >>> l(y_true, y_pred).numpy()\n",
      "     |  0.108\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> l(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()\n",
      "     |  0.087\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> l = tf.keras.losses.LogCosh(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> l(y_true, y_pred).numpy()\n",
      "     |  0.217\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> l = tf.keras.losses.LogCosh(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> l(y_true, y_pred).numpy()\n",
      "     |  array([0.217, 0.], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd', loss=tf.keras.losses.LogCosh())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LogCosh\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reduction='auto', name='log_cosh')\n",
      "     |      Initializes `LogCosh` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance. Defaults to 'log_cosh'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Loss(builtins.object)\n",
      "     |  Loss(reduction='auto', name=None)\n",
      "     |  \n",
      "     |  Loss base class.\n",
      "     |  \n",
      "     |  To be implemented by subclasses:\n",
      "     |  * `call()`: Contains the logic for loss calculation using `y_true`,\n",
      "     |    `y_pred`.\n",
      "     |  \n",
      "     |  Example subclass implementation:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  class MeanSquaredError(Loss):\n",
      "     |  \n",
      "     |    def call(self, y_true, y_pred):\n",
      "     |      return tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1)\n",
      "     |  ```\n",
      "     |  \n",
      "     |  When used with `tf.distribute.Strategy`, outside of built-in training loops\n",
      "     |  such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction\n",
      "     |  types, and reduce losses explicitly in your training loop. Using 'AUTO' or\n",
      "     |  'SUM_OVER_BATCH_SIZE' will raise an error.\n",
      "     |  \n",
      "     |  Please see this custom training [tutorial](\n",
      "     |    https://www.tensorflow.org/tutorials/distribute/custom_training) for more\n",
      "     |  details on this.\n",
      "     |  \n",
      "     |  You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  with strategy.scope():\n",
      "     |    loss_obj = tf.keras.losses.CategoricalCrossentropy(\n",
      "     |        reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |    ....\n",
      "     |    loss = (tf.reduce_sum(loss_obj(labels, predictions)) *\n",
      "     |            (1. / global_batch_size))\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  __init__(self, reduction='auto', name=None)\n",
      "     |      Initializes `Loss` class.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE`\n",
      "     |          will raise an error. Please see this custom training [tutorial](\n",
      "     |            https://www.tensorflow.org/tutorials/distribute/custom_training)\n",
      "     |            for more details.\n",
      "     |        name: Optional name for the instance.\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values with the shape `[batch_size, d0, .. dN-1]`.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LossFunctionWrapper(Loss)\n",
      "     |  LossFunctionWrapper(fn, reduction='auto', name=None, **kwargs)\n",
      "     |  \n",
      "     |  Wraps a loss function in the `Loss` class.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fn, reduction='auto', name=None, **kwargs)\n",
      "     |      Initializes `LossFunctionWrapper` class.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        fn: The loss function to wrap, with signature `fn(y_true, y_pred,\n",
      "     |          **kwargs)`.\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance.\n",
      "     |        **kwargs: The keyword arguments that are passed on to `fn`.\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MeanAbsoluteError(LossFunctionWrapper)\n",
      "     |  MeanAbsoluteError(reduction='auto', name='mean_absolute_error')\n",
      "     |  \n",
      "     |  Computes the mean of absolute difference between labels and predictions.\n",
      "     |  \n",
      "     |  `loss = abs(y_true - y_pred)`\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[0., 1.], [0., 0.]]\n",
      "     |  >>> y_pred = [[1., 1.], [1., 0.]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> mae = tf.keras.losses.MeanAbsoluteError()\n",
      "     |  >>> mae(y_true, y_pred).numpy()\n",
      "     |  0.5\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> mae(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()\n",
      "     |  0.25\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> mae = tf.keras.losses.MeanAbsoluteError(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> mae(y_true, y_pred).numpy()\n",
      "     |  1.0\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> mae = tf.keras.losses.MeanAbsoluteError(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> mae(y_true, y_pred).numpy()\n",
      "     |  array([0.5, 0.5], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsoluteError())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MeanAbsoluteError\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reduction='auto', name='mean_absolute_error')\n",
      "     |      Initializes `MeanAbsoluteError` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance. Defaults to\n",
      "     |          'mean_absolute_error'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MeanAbsolutePercentageError(LossFunctionWrapper)\n",
      "     |  MeanAbsolutePercentageError(reduction='auto', name='mean_absolute_percentage_error')\n",
      "     |  \n",
      "     |  Computes the mean absolute percentage error between `y_true` and `y_pred`.\n",
      "     |  \n",
      "     |  Formula:\n",
      "     |  \n",
      "     |  `loss = 100 * abs((y_true - y_pred) / y_true)`\n",
      "     |  \n",
      "     |  Note that to avoid dividing by zero, a small epsilon value\n",
      "     |  is added to the denominator.\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[2., 1.], [2., 3.]]\n",
      "     |  >>> y_pred = [[1., 1.], [1., 0.]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> mape = tf.keras.losses.MeanAbsolutePercentageError()\n",
      "     |  >>> mape(y_true, y_pred).numpy()\n",
      "     |  50.\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> mape(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()\n",
      "     |  20.\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> mape = tf.keras.losses.MeanAbsolutePercentageError(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> mape(y_true, y_pred).numpy()\n",
      "     |  100.\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> mape = tf.keras.losses.MeanAbsolutePercentageError(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> mape(y_true, y_pred).numpy()\n",
      "     |  array([25., 75.], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd',\n",
      "     |                loss=tf.keras.losses.MeanAbsolutePercentageError())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MeanAbsolutePercentageError\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reduction='auto', name='mean_absolute_percentage_error')\n",
      "     |      Initializes `MeanAbsolutePercentageError` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance. Defaults to\n",
      "     |          'mean_absolute_percentage_error'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MeanSquaredError(LossFunctionWrapper)\n",
      "     |  MeanSquaredError(reduction='auto', name='mean_squared_error')\n",
      "     |  \n",
      "     |  Computes the mean of squares of errors between labels and predictions.\n",
      "     |  \n",
      "     |  `loss = square(y_true - y_pred)`\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[0., 1.], [0., 0.]]\n",
      "     |  >>> y_pred = [[1., 1.], [1., 0.]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> mse = tf.keras.losses.MeanSquaredError()\n",
      "     |  >>> mse(y_true, y_pred).numpy()\n",
      "     |  0.5\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> mse(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()\n",
      "     |  0.25\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> mse = tf.keras.losses.MeanSquaredError(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> mse(y_true, y_pred).numpy()\n",
      "     |  1.0\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> mse = tf.keras.losses.MeanSquaredError(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> mse(y_true, y_pred).numpy()\n",
      "     |  array([0.5, 0.5], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredError())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MeanSquaredError\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reduction='auto', name='mean_squared_error')\n",
      "     |      Initializes `MeanSquaredError` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance. Defaults to\n",
      "     |          'mean_squared_error'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MeanSquaredLogarithmicError(LossFunctionWrapper)\n",
      "     |  MeanSquaredLogarithmicError(reduction='auto', name='mean_squared_logarithmic_error')\n",
      "     |  \n",
      "     |  Computes the mean squared logarithmic error between `y_true` and `y_pred`.\n",
      "     |  \n",
      "     |  `loss = square(log(y_true + 1.) - log(y_pred + 1.))`\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[0., 1.], [0., 0.]]\n",
      "     |  >>> y_pred = [[1., 1.], [1., 0.]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> msle = tf.keras.losses.MeanSquaredLogarithmicError()\n",
      "     |  >>> msle(y_true, y_pred).numpy()\n",
      "     |  0.240\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> msle(y_true, y_pred, sample_weight=[0.7, 0.3]).numpy()\n",
      "     |  0.120\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> msle = tf.keras.losses.MeanSquaredLogarithmicError(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> msle(y_true, y_pred).numpy()\n",
      "     |  0.480\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> msle = tf.keras.losses.MeanSquaredLogarithmicError(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> msle(y_true, y_pred).numpy()\n",
      "     |  array([0.240, 0.240], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd',\n",
      "     |                loss=tf.keras.losses.MeanSquaredLogarithmicError())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MeanSquaredLogarithmicError\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reduction='auto', name='mean_squared_logarithmic_error')\n",
      "     |      Initializes `MeanSquaredLogarithmicError` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance. Defaults to\n",
      "     |          'mean_squared_logarithmic_error'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Poisson(LossFunctionWrapper)\n",
      "     |  Poisson(reduction='auto', name='poisson')\n",
      "     |  \n",
      "     |  Computes the Poisson loss between `y_true` and `y_pred`.\n",
      "     |  \n",
      "     |  `loss = y_pred - y_true * log(y_pred)`\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[0., 1.], [0., 0.]]\n",
      "     |  >>> y_pred = [[1., 1.], [0., 0.]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> p = tf.keras.losses.Poisson()\n",
      "     |  >>> p(y_true, y_pred).numpy()\n",
      "     |  0.5\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> p(y_true, y_pred, sample_weight=[0.8, 0.2]).numpy()\n",
      "     |  0.4\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> p = tf.keras.losses.Poisson(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> p(y_true, y_pred).numpy()\n",
      "     |  0.999\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> p = tf.keras.losses.Poisson(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> p(y_true, y_pred).numpy()\n",
      "     |  array([0.999, 0.], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd', loss=tf.keras.losses.Poisson())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Poisson\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reduction='auto', name='poisson')\n",
      "     |      Initializes `Poisson` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance. Defaults to 'poisson'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SparseCategoricalCrossentropy(LossFunctionWrapper)\n",
      "     |  SparseCategoricalCrossentropy(from_logits=False, ignore_class=None, reduction='auto', name='sparse_categorical_crossentropy')\n",
      "     |  \n",
      "     |  Computes the crossentropy loss between the labels and predictions.\n",
      "     |  \n",
      "     |  Use this crossentropy loss function when there are two or more label\n",
      "     |  classes.  We expect labels to be provided as integers. If you want to\n",
      "     |  provide labels using `one-hot` representation, please use\n",
      "     |  `CategoricalCrossentropy` loss.  There should be `# classes` floating point\n",
      "     |  values per feature for `y_pred` and a single floating point value per\n",
      "     |  feature for `y_true`.\n",
      "     |  \n",
      "     |  In the snippet below, there is a single floating point value per example for\n",
      "     |  `y_true` and `# classes` floating pointing values per example for `y_pred`.\n",
      "     |  The shape of `y_true` is `[batch_size]` and the shape of `y_pred` is\n",
      "     |  `[batch_size, num_classes]`.\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [1, 2]\n",
      "     |  >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
      "     |  >>> scce(y_true, y_pred).numpy()\n",
      "     |  1.177\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> scce(y_true, y_pred, sample_weight=tf.constant([0.3, 0.7])).numpy()\n",
      "     |  0.814\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> scce = tf.keras.losses.SparseCategoricalCrossentropy(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> scce(y_true, y_pred).numpy()\n",
      "     |  2.354\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> scce = tf.keras.losses.SparseCategoricalCrossentropy(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> scce(y_true, y_pred).numpy()\n",
      "     |  array([0.0513, 2.303], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd',\n",
      "     |                loss=tf.keras.losses.SparseCategoricalCrossentropy())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SparseCategoricalCrossentropy\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, from_logits=False, ignore_class=None, reduction='auto', name='sparse_categorical_crossentropy')\n",
      "     |      Initializes `SparseCategoricalCrossentropy` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        from_logits: Whether `y_pred` is expected to be a logits tensor. By\n",
      "     |          default, we assume that `y_pred` encodes a probability distribution.\n",
      "     |        ignore_class: Optional integer. The ID of a class to be ignored during\n",
      "     |          loss computation. This is useful, for example, in segmentation\n",
      "     |          problems featuring a \"void\" class (commonly -1 or 255) in\n",
      "     |          segmentation maps.\n",
      "     |          By default (`ignore_class=None`), all classes are considered.\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance. Defaults to\n",
      "     |          'sparse_categorical_crossentropy'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SquaredHinge(LossFunctionWrapper)\n",
      "     |  SquaredHinge(reduction='auto', name='squared_hinge')\n",
      "     |  \n",
      "     |  Computes the squared hinge loss between `y_true` and `y_pred`.\n",
      "     |  \n",
      "     |  `loss = square(maximum(1 - y_true * y_pred, 0))`\n",
      "     |  \n",
      "     |  `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n",
      "     |  provided we will convert them to -1 or 1.\n",
      "     |  \n",
      "     |  Standalone usage:\n",
      "     |  \n",
      "     |  >>> y_true = [[0., 1.], [0., 0.]]\n",
      "     |  >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
      "     |  >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n",
      "     |  >>> h = tf.keras.losses.SquaredHinge()\n",
      "     |  >>> h(y_true, y_pred).numpy()\n",
      "     |  1.86\n",
      "     |  \n",
      "     |  >>> # Calling with 'sample_weight'.\n",
      "     |  >>> h(y_true, y_pred, sample_weight=[1, 0]).numpy()\n",
      "     |  0.73\n",
      "     |  \n",
      "     |  >>> # Using 'sum' reduction type.\n",
      "     |  >>> h = tf.keras.losses.SquaredHinge(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.SUM)\n",
      "     |  >>> h(y_true, y_pred).numpy()\n",
      "     |  3.72\n",
      "     |  \n",
      "     |  >>> # Using 'none' reduction type.\n",
      "     |  >>> h = tf.keras.losses.SquaredHinge(\n",
      "     |  ...     reduction=tf.keras.losses.Reduction.NONE)\n",
      "     |  >>> h(y_true, y_pred).numpy()\n",
      "     |  array([1.46, 2.26], dtype=float32)\n",
      "     |  \n",
      "     |  Usage with the `compile()` API:\n",
      "     |  \n",
      "     |  ```python\n",
      "     |  model.compile(optimizer='sgd', loss=tf.keras.losses.SquaredHinge())\n",
      "     |  ```\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SquaredHinge\n",
      "     |      LossFunctionWrapper\n",
      "     |      Loss\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, reduction='auto', name='squared_hinge')\n",
      "     |      Initializes `SquaredHinge` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        reduction: Type of `tf.keras.losses.Reduction` to apply to\n",
      "     |          loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n",
      "     |          option will be determined by the usage context. For almost all cases\n",
      "     |          this defaults to `SUM_OVER_BATCH_SIZE`. When used with\n",
      "     |          `tf.distribute.Strategy`, outside of built-in training loops such as\n",
      "     |          `tf.keras` `compile` and `fit`, using `AUTO` or\n",
      "     |          `SUM_OVER_BATCH_SIZE` will raise an error. Please see this custom\n",
      "     |          training [tutorial](\n",
      "     |          https://www.tensorflow.org/tutorials/distribute/custom_training) for\n",
      "     |          more details.\n",
      "     |        name: Optional name for the instance. Defaults to 'squared_hinge'.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  call(self, y_true, y_pred)\n",
      "     |      Invokes the `LossFunctionWrapper` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values.\n",
      "     |        y_pred: The predicted values.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Loss values per sample.\n",
      "     |  \n",
      "     |  get_config(self)\n",
      "     |      Returns the config dictionary for a `Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from LossFunctionWrapper:\n",
      "     |  \n",
      "     |  from_config(config) from builtins.type\n",
      "     |      Instantiates a `Loss` from its config (output of `get_config()`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          config: Output of `get_config()`.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A `keras.losses.Loss` instance.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Loss:\n",
      "     |  \n",
      "     |  __call__(self, y_true, y_pred, sample_weight=None)\n",
      "     |      Invokes the `Loss` instance.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |        y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except\n",
      "     |          sparse loss functions such as sparse categorical crossentropy where\n",
      "     |          shape = `[batch_size, d0, .. dN-1]`\n",
      "     |        y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`\n",
      "     |        sample_weight: Optional `sample_weight` acts as a coefficient for the\n",
      "     |          loss. If a scalar is provided, then the loss is simply scaled by the\n",
      "     |          given value. If `sample_weight` is a tensor of size `[batch_size]`,\n",
      "     |          then the total loss for each sample of the batch is rescaled by the\n",
      "     |          corresponding element in the `sample_weight` vector. If the shape of\n",
      "     |          `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be\n",
      "     |          broadcasted to this shape), then each loss element of `y_pred` is\n",
      "     |          scaled by the corresponding value of `sample_weight`. (Note\n",
      "     |          on`dN-1`: all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |        Weighted loss float `Tensor`. If `reduction` is `NONE`, this has\n",
      "     |          shape `[batch_size, d0, .. dN-1]`; otherwise, it is scalar. (Note\n",
      "     |          `dN-1` because all loss functions reduce by 1 dimension, usually\n",
      "     |          axis=-1.)\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |        ValueError: If the shape of `sample_weight` is invalid.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from Loss:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    BCE = binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1)\n",
      "        Computes the binary crossentropy loss.\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = [[0, 1], [0, 0]]\n",
      "        >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
      "        >>> loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> loss.numpy()\n",
      "        array([0.916 , 0.714], dtype=float32)\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "          from_logits: Whether `y_pred` is expected to be a logits tensor. By\n",
      "            default, we assume that `y_pred` encodes a probability distribution.\n",
      "          label_smoothing: Float in [0, 1]. If > `0` then smooth the labels by\n",
      "            squeezing them towards 0.5 That is, using `1. - 0.5 * label_smoothing`\n",
      "            for the target class and `0.5 * label_smoothing` for the non-target\n",
      "            class.\n",
      "          axis: The axis along which the mean is computed. Defaults to -1.\n",
      "        \n",
      "        Returns:\n",
      "          Binary crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    KLD = kl_divergence(y_true, y_pred)\n",
      "        Computes Kullback-Leibler divergence loss between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = y_true * log(y_true / y_pred)`\n",
      "        \n",
      "        See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> y_true = tf.keras.backend.clip(y_true, 1e-7, 1)\n",
      "        >>> y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Tensor of true targets.\n",
      "          y_pred: Tensor of predicted targets.\n",
      "        \n",
      "        Returns:\n",
      "          A `Tensor` with loss.\n",
      "        \n",
      "        Raises:\n",
      "          TypeError: If `y_true` cannot be cast to the `y_pred.dtype`.\n",
      "    \n",
      "    MAE = mean_absolute_error(y_true, y_pred)\n",
      "        Computes the mean absolute error between labels and predictions.\n",
      "        \n",
      "        `loss = mean(abs(y_true - y_pred), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    MAPE = mean_absolute_percentage_error(y_true, y_pred)\n",
      "        Computes the mean absolute percentage error between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.random(size=(2, 3))\n",
      "        >>> y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(),\n",
      "        ...     100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Mean absolute percentage error values. shape = `[batch_size, d0, ..\n",
      "          dN-1]`.\n",
      "    \n",
      "    MSE = mean_squared_error(y_true, y_pred)\n",
      "        Computes the mean squared error between labels and predictions.\n",
      "        \n",
      "        After computing the squared distance between the inputs, the mean value over\n",
      "        the last dimension is returned.\n",
      "        \n",
      "        `loss = mean(square(y_true - y_pred), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Mean squared error values. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    MSLE = mean_squared_logarithmic_error(y_true, y_pred)\n",
      "        Computes the mean squared logarithmic error between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> y_true = np.maximum(y_true, 1e-7)\n",
      "        >>> y_pred = np.maximum(y_pred, 1e-7)\n",
      "        >>> assert np.allclose(\n",
      "        ...     loss.numpy(),\n",
      "        ...     np.mean(\n",
      "        ...         np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Mean squared logarithmic error values. shape = `[batch_size, d0, ..\n",
      "          dN-1]`.\n",
      "    \n",
      "    bce = binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1)\n",
      "        Computes the binary crossentropy loss.\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = [[0, 1], [0, 0]]\n",
      "        >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
      "        >>> loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> loss.numpy()\n",
      "        array([0.916 , 0.714], dtype=float32)\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "          from_logits: Whether `y_pred` is expected to be a logits tensor. By\n",
      "            default, we assume that `y_pred` encodes a probability distribution.\n",
      "          label_smoothing: Float in [0, 1]. If > `0` then smooth the labels by\n",
      "            squeezing them towards 0.5 That is, using `1. - 0.5 * label_smoothing`\n",
      "            for the target class and `0.5 * label_smoothing` for the non-target\n",
      "            class.\n",
      "          axis: The axis along which the mean is computed. Defaults to -1.\n",
      "        \n",
      "        Returns:\n",
      "          Binary crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    binary_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1)\n",
      "        Computes the binary crossentropy loss.\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = [[0, 1], [0, 0]]\n",
      "        >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
      "        >>> loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> loss.numpy()\n",
      "        array([0.916 , 0.714], dtype=float32)\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "          from_logits: Whether `y_pred` is expected to be a logits tensor. By\n",
      "            default, we assume that `y_pred` encodes a probability distribution.\n",
      "          label_smoothing: Float in [0, 1]. If > `0` then smooth the labels by\n",
      "            squeezing them towards 0.5 That is, using `1. - 0.5 * label_smoothing`\n",
      "            for the target class and `0.5 * label_smoothing` for the non-target\n",
      "            class.\n",
      "          axis: The axis along which the mean is computed. Defaults to -1.\n",
      "        \n",
      "        Returns:\n",
      "          Binary crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    binary_focal_crossentropy(y_true, y_pred, apply_class_balancing=False, alpha=0.25, gamma=2.0, from_logits=False, label_smoothing=0.0, axis=-1)\n",
      "        Computes the binary focal crossentropy loss.\n",
      "        \n",
      "        According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n",
      "        helps to apply a focal factor to down-weight easy examples and focus more on\n",
      "        hard examples. By default, the focal tensor is computed as follows:\n",
      "        \n",
      "        `focal_factor = (1 - output)**gamma` for class 1\n",
      "        `focal_factor = output**gamma` for class 0\n",
      "        where `gamma` is a focusing parameter. When `gamma` = 0, there is no focal\n",
      "        effect on the binary crossentropy loss.\n",
      "        \n",
      "        If `apply_class_balancing == True`, this function also takes into account a\n",
      "        weight balancing factor for the binary classes 0 and 1 as follows:\n",
      "        \n",
      "        `weight = alpha` for class 1 (`target == 1`)\n",
      "        `weight = 1 - alpha` for class 0\n",
      "        where `alpha` is a float in the range of `[0, 1]`.\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = [[0, 1], [0, 0]]\n",
      "        >>> y_pred = [[0.6, 0.4], [0.4, 0.6]]\n",
      "        >>> loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,\n",
      "        ...                                                  gamma=2)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> loss.numpy()\n",
      "        array([0.330, 0.206], dtype=float32)\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values, of shape `(batch_size, d0, .. dN)`.\n",
      "          y_pred: The predicted values, of shape `(batch_size, d0, .. dN)`.\n",
      "          apply_class_balancing: A bool, whether to apply weight balancing on the\n",
      "            binary classes 0 and 1.\n",
      "          alpha: A weight balancing factor for class 1, default is `0.25` as\n",
      "            mentioned in the reference. The weight for class 0 is `1.0 - alpha`.\n",
      "          gamma: A focusing parameter, default is `2.0` as mentioned in the\n",
      "            reference.\n",
      "          from_logits: Whether `y_pred` is expected to be a logits tensor. By\n",
      "            default, we assume that `y_pred` encodes a probability distribution.\n",
      "          label_smoothing: Float in `[0, 1]`. If higher than 0 then smooth the\n",
      "            labels by squeezing them towards `0.5`, i.e., using `1. - 0.5 *\n",
      "            label_smoothing` for the target class and `0.5 * label_smoothing` for\n",
      "            the non-target class.\n",
      "          axis: The axis along which the mean is computed. Defaults to `-1`.\n",
      "        \n",
      "        Returns:\n",
      "          Binary focal crossentropy loss value. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1)\n",
      "        Computes the categorical crossentropy loss.\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = [[0, 1, 0], [0, 0, 1]]\n",
      "        >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
      "        >>> loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> loss.numpy()\n",
      "        array([0.0513, 2.303], dtype=float32)\n",
      "        \n",
      "        Args:\n",
      "          y_true: Tensor of one-hot true targets.\n",
      "          y_pred: Tensor of predicted targets.\n",
      "          from_logits: Whether `y_pred` is expected to be a logits tensor. By\n",
      "            default, we assume that `y_pred` encodes a probability distribution.\n",
      "          label_smoothing: Float in [0, 1]. If > `0` then smooth the labels. For\n",
      "            example, if `0.1`, use `0.1 / num_classes` for non-target labels\n",
      "            and `0.9 + 0.1 / num_classes` for target labels.\n",
      "          axis: Defaults to -1. The dimension along which the entropy is\n",
      "            computed.\n",
      "        \n",
      "        Returns:\n",
      "          Categorical crossentropy loss value.\n",
      "    \n",
      "    categorical_hinge(y_true, y_pred)\n",
      "        Computes the categorical hinge loss between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = maximum(neg - pos + 1, 0)`\n",
      "        where `neg=maximum((1-y_true)*y_pred) and pos=sum(y_true*y_pred)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 3, size=(2,))\n",
      "        >>> y_true = tf.keras.utils.to_categorical(y_true, num_classes=3)\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.categorical_hinge(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> pos = np.sum(y_true * y_pred, axis=-1)\n",
      "        >>> neg = np.amax((1. - y_true) * y_pred, axis=-1)\n",
      "        >>> assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.))\n",
      "        \n",
      "        Args:\n",
      "          y_true: The ground truth values. `y_true` values are expected to be\n",
      "          either `{-1, +1}` or `{0, 1}` (i.e. a one-hot-encoded tensor).\n",
      "          y_pred: The predicted values.\n",
      "        \n",
      "        Returns:\n",
      "          Categorical hinge loss values.\n",
      "    \n",
      "    cosine_similarity(y_true, y_pred, axis=-1)\n",
      "        Computes the cosine similarity between labels and predictions.\n",
      "        \n",
      "        Note that it is a number between -1 and 1. When it is a negative number\n",
      "        between -1 and 0, 0 indicates orthogonality and values closer to -1\n",
      "        indicate greater similarity. The values closer to 1 indicate greater\n",
      "        dissimilarity. This makes it usable as a loss function in a setting\n",
      "        where you try to maximize the proximity between predictions and\n",
      "        targets. If either `y_true` or `y_pred` is a zero vector, cosine\n",
      "        similarity will be 0 regardless of the proximity between predictions\n",
      "        and targets.\n",
      "        \n",
      "        `loss = -sum(l2_norm(y_true) * l2_norm(y_pred))`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = [[0., 1.], [1., 1.], [1., 1.]]\n",
      "        >>> y_pred = [[1., 0.], [1., 1.], [-1., -1.]]\n",
      "        >>> loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)\n",
      "        >>> loss.numpy()\n",
      "        array([-0., -0.999, 0.999], dtype=float32)\n",
      "        \n",
      "        Args:\n",
      "          y_true: Tensor of true targets.\n",
      "          y_pred: Tensor of predicted targets.\n",
      "          axis: Axis along which to determine similarity.\n",
      "        \n",
      "        Returns:\n",
      "          Cosine similarity tensor.\n",
      "    \n",
      "    deserialize(name, custom_objects=None)\n",
      "        Deserializes a serialized loss class/function instance.\n",
      "        \n",
      "        Args:\n",
      "            name: Loss configuration.\n",
      "            custom_objects: Optional dictionary mapping names (strings) to custom\n",
      "              objects (classes and functions) to be considered during\n",
      "              deserialization.\n",
      "        \n",
      "        Returns:\n",
      "            A Keras `Loss` instance or a loss function.\n",
      "    \n",
      "    get(identifier)\n",
      "        Retrieves a Keras loss as a `function`/`Loss` class instance.\n",
      "        \n",
      "        The `identifier` may be the string name of a loss function or `Loss` class.\n",
      "        \n",
      "        >>> loss = tf.keras.losses.get(\"categorical_crossentropy\")\n",
      "        >>> type(loss)\n",
      "        <class 'function'>\n",
      "        >>> loss = tf.keras.losses.get(\"CategoricalCrossentropy\")\n",
      "        >>> type(loss)\n",
      "        <class '...keras.losses.CategoricalCrossentropy'>\n",
      "        \n",
      "        You can also specify `config` of the loss to this function by passing dict\n",
      "        containing `class_name` and `config` as an identifier. Also note that the\n",
      "        `class_name` must map to a `Loss` class\n",
      "        \n",
      "        >>> identifier = {\"class_name\": \"CategoricalCrossentropy\",\n",
      "        ...               \"config\": {\"from_logits\": True}}\n",
      "        >>> loss = tf.keras.losses.get(identifier)\n",
      "        >>> type(loss)\n",
      "        <class '...keras.losses.CategoricalCrossentropy'>\n",
      "        \n",
      "        Args:\n",
      "          identifier: A loss identifier. One of None or string name of a loss\n",
      "            function/class or loss configuration dictionary or a loss function or a\n",
      "            loss class instance.\n",
      "        \n",
      "        Returns:\n",
      "          A Keras loss as a `function`/ `Loss` class instance.\n",
      "        \n",
      "        Raises:\n",
      "          ValueError: If `identifier` cannot be interpreted.\n",
      "    \n",
      "    hinge(y_true, y_pred)\n",
      "        Computes the hinge loss between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = mean(maximum(1 - y_true * y_pred, 0), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.hinge(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(),\n",
      "        ...     np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: The ground truth values. `y_true` values are expected to be -1 or\n",
      "            1. If binary (0 or 1) labels are provided they will be converted to -1\n",
      "            or 1. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Hinge loss values. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    huber(y_true, y_pred, delta=1.0)\n",
      "        Computes Huber loss value.\n",
      "        \n",
      "        For each value x in `error = y_true - y_pred`:\n",
      "        \n",
      "        ```\n",
      "        loss = 0.5 * x^2                  if |x| <= d\n",
      "        loss = d * |x| - 0.5 * d^2        if |x| > d\n",
      "        ```\n",
      "        where d is `delta`. See: https://en.wikipedia.org/wiki/Huber_loss\n",
      "        \n",
      "        Args:\n",
      "          y_true: tensor of true targets.\n",
      "          y_pred: tensor of predicted targets.\n",
      "          delta: A float, the point where the Huber loss function changes from a\n",
      "            quadratic to linear.\n",
      "        \n",
      "        Returns:\n",
      "          Tensor with one scalar loss entry per sample.\n",
      "    \n",
      "    huber_loss = huber(y_true, y_pred, delta=1.0)\n",
      "        Computes Huber loss value.\n",
      "        \n",
      "        For each value x in `error = y_true - y_pred`:\n",
      "        \n",
      "        ```\n",
      "        loss = 0.5 * x^2                  if |x| <= d\n",
      "        loss = d * |x| - 0.5 * d^2        if |x| > d\n",
      "        ```\n",
      "        where d is `delta`. See: https://en.wikipedia.org/wiki/Huber_loss\n",
      "        \n",
      "        Args:\n",
      "          y_true: tensor of true targets.\n",
      "          y_pred: tensor of predicted targets.\n",
      "          delta: A float, the point where the Huber loss function changes from a\n",
      "            quadratic to linear.\n",
      "        \n",
      "        Returns:\n",
      "          Tensor with one scalar loss entry per sample.\n",
      "    \n",
      "    is_categorical_crossentropy(loss)\n",
      "    \n",
      "    kl_divergence(y_true, y_pred)\n",
      "        Computes Kullback-Leibler divergence loss between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = y_true * log(y_true / y_pred)`\n",
      "        \n",
      "        See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> y_true = tf.keras.backend.clip(y_true, 1e-7, 1)\n",
      "        >>> y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Tensor of true targets.\n",
      "          y_pred: Tensor of predicted targets.\n",
      "        \n",
      "        Returns:\n",
      "          A `Tensor` with loss.\n",
      "        \n",
      "        Raises:\n",
      "          TypeError: If `y_true` cannot be cast to the `y_pred.dtype`.\n",
      "    \n",
      "    kld = kl_divergence(y_true, y_pred)\n",
      "        Computes Kullback-Leibler divergence loss between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = y_true * log(y_true / y_pred)`\n",
      "        \n",
      "        See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> y_true = tf.keras.backend.clip(y_true, 1e-7, 1)\n",
      "        >>> y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Tensor of true targets.\n",
      "          y_pred: Tensor of predicted targets.\n",
      "        \n",
      "        Returns:\n",
      "          A `Tensor` with loss.\n",
      "        \n",
      "        Raises:\n",
      "          TypeError: If `y_true` cannot be cast to the `y_pred.dtype`.\n",
      "    \n",
      "    kullback_leibler_divergence = kl_divergence(y_true, y_pred)\n",
      "        Computes Kullback-Leibler divergence loss between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = y_true * log(y_true / y_pred)`\n",
      "        \n",
      "        See: https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> y_true = tf.keras.backend.clip(y_true, 1e-7, 1)\n",
      "        >>> y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Tensor of true targets.\n",
      "          y_pred: Tensor of predicted targets.\n",
      "        \n",
      "        Returns:\n",
      "          A `Tensor` with loss.\n",
      "        \n",
      "        Raises:\n",
      "          TypeError: If `y_true` cannot be cast to the `y_pred.dtype`.\n",
      "    \n",
      "    log_cosh(y_true, y_pred)\n",
      "        Logarithm of the hyperbolic cosine of the prediction error.\n",
      "        \n",
      "        `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n",
      "        to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n",
      "        like the mean squared error, but will not be so strongly affected by the\n",
      "        occasional wildly incorrect prediction.\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.random(size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.logcosh(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> x = y_pred - y_true\n",
      "        >>> assert np.allclose(\n",
      "        ...     loss.numpy(),\n",
      "        ...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),\n",
      "        ...             axis=-1),\n",
      "        ...     atol=1e-5)\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Logcosh error values. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    logcosh = log_cosh(y_true, y_pred)\n",
      "        Logarithm of the hyperbolic cosine of the prediction error.\n",
      "        \n",
      "        `log(cosh(x))` is approximately equal to `(x ** 2) / 2` for small `x` and\n",
      "        to `abs(x) - log(2)` for large `x`. This means that 'logcosh' works mostly\n",
      "        like the mean squared error, but will not be so strongly affected by the\n",
      "        occasional wildly incorrect prediction.\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.random(size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.logcosh(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> x = y_pred - y_true\n",
      "        >>> assert np.allclose(\n",
      "        ...     loss.numpy(),\n",
      "        ...     np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),\n",
      "        ...             axis=-1),\n",
      "        ...     atol=1e-5)\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Logcosh error values. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    mae = mean_absolute_error(y_true, y_pred)\n",
      "        Computes the mean absolute error between labels and predictions.\n",
      "        \n",
      "        `loss = mean(abs(y_true - y_pred), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
      "        Computes the mean absolute percentage error between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.random(size=(2, 3))\n",
      "        >>> y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(),\n",
      "        ...     100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Mean absolute percentage error values. shape = `[batch_size, d0, ..\n",
      "          dN-1]`.\n",
      "    \n",
      "    mean_absolute_error(y_true, y_pred)\n",
      "        Computes the mean absolute error between labels and predictions.\n",
      "        \n",
      "        `loss = mean(abs(y_true - y_pred), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Mean absolute error values. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    mean_absolute_percentage_error(y_true, y_pred)\n",
      "        Computes the mean absolute percentage error between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.random(size=(2, 3))\n",
      "        >>> y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(),\n",
      "        ...     100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Mean absolute percentage error values. shape = `[batch_size, d0, ..\n",
      "          dN-1]`.\n",
      "    \n",
      "    mean_squared_error(y_true, y_pred)\n",
      "        Computes the mean squared error between labels and predictions.\n",
      "        \n",
      "        After computing the squared distance between the inputs, the mean value over\n",
      "        the last dimension is returned.\n",
      "        \n",
      "        `loss = mean(square(y_true - y_pred), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Mean squared error values. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    mean_squared_logarithmic_error(y_true, y_pred)\n",
      "        Computes the mean squared logarithmic error between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> y_true = np.maximum(y_true, 1e-7)\n",
      "        >>> y_pred = np.maximum(y_pred, 1e-7)\n",
      "        >>> assert np.allclose(\n",
      "        ...     loss.numpy(),\n",
      "        ...     np.mean(\n",
      "        ...         np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Mean squared logarithmic error values. shape = `[batch_size, d0, ..\n",
      "          dN-1]`.\n",
      "    \n",
      "    mse = mean_squared_error(y_true, y_pred)\n",
      "        Computes the mean squared error between labels and predictions.\n",
      "        \n",
      "        After computing the squared distance between the inputs, the mean value over\n",
      "        the last dimension is returned.\n",
      "        \n",
      "        `loss = mean(square(y_true - y_pred), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Mean squared error values. shape = `[batch_size, d0, .. dN-1]`.\n",
      "    \n",
      "    msle = mean_squared_logarithmic_error(y_true, y_pred)\n",
      "        Computes the mean squared logarithmic error between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> y_true = np.maximum(y_true, 1e-7)\n",
      "        >>> y_pred = np.maximum(y_pred, 1e-7)\n",
      "        >>> assert np.allclose(\n",
      "        ...     loss.numpy(),\n",
      "        ...     np.mean(\n",
      "        ...         np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "          Mean squared logarithmic error values. shape = `[batch_size, d0, ..\n",
      "          dN-1]`.\n",
      "    \n",
      "    poisson(y_true, y_pred)\n",
      "        Computes the Poisson loss between y_true and y_pred.\n",
      "        \n",
      "        The Poisson loss is the mean of the elements of the `Tensor`\n",
      "        `y_pred - y_true * log(y_pred)`.\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.randint(0, 2, size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.poisson(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> y_pred = y_pred + 1e-7\n",
      "        >>> assert np.allclose(\n",
      "        ...     loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),\n",
      "        ...     atol=1e-5)\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "           Poisson loss value. shape = `[batch_size, d0, .. dN-1]`.\n",
      "        \n",
      "        Raises:\n",
      "          InvalidArgumentError: If `y_true` and `y_pred` have incompatible shapes.\n",
      "    \n",
      "    serialize(loss)\n",
      "        Serializes loss function or `Loss` instance.\n",
      "        \n",
      "        Args:\n",
      "          loss: A Keras `Loss` instance or a loss function.\n",
      "        \n",
      "        Returns:\n",
      "          Loss configuration dictionary.\n",
      "    \n",
      "    sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1, ignore_class=None)\n",
      "        Computes the sparse categorical crossentropy loss.\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = [1, 2]\n",
      "        >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
      "        >>> loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> loss.numpy()\n",
      "        array([0.0513, 2.303], dtype=float32)\n",
      "        \n",
      "        >>> y_true = [[[ 0,  2],\n",
      "        ...            [-1, -1]],\n",
      "        ...           [[ 0,  2],\n",
      "        ...            [-1, -1]]]\n",
      "        >>> y_pred = [[[[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n",
      "        ...             [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]],\n",
      "        ...           [[[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n",
      "        ...            [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]]]\n",
      "        >>> loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
      "        ...   y_true, y_pred, ignore_class=-1)\n",
      "        >>> loss.numpy()\n",
      "        array([[[2.3841855e-07, 2.3841855e-07],\n",
      "                [0.0000000e+00, 0.0000000e+00]],\n",
      "               [[2.3841855e-07, 6.9314730e-01],\n",
      "                [0.0000000e+00, 0.0000000e+00]]], dtype=float32)\n",
      "        \n",
      "        Args:\n",
      "          y_true: Ground truth values.\n",
      "          y_pred: The predicted values.\n",
      "          from_logits: Whether `y_pred` is expected to be a logits tensor. By\n",
      "            default, we assume that `y_pred` encodes a probability distribution.\n",
      "          axis: Defaults to -1. The dimension along which the entropy is\n",
      "            computed.\n",
      "          ignore_class: Optional integer. The ID of a class to be ignored during\n",
      "            loss computation. This is useful, for example, in segmentation\n",
      "            problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n",
      "            maps. By default (`ignore_class=None`), all classes are considered.\n",
      "        \n",
      "        Returns:\n",
      "          Sparse categorical crossentropy loss value.\n",
      "    \n",
      "    squared_hinge(y_true, y_pred)\n",
      "        Computes the squared hinge loss between `y_true` and `y_pred`.\n",
      "        \n",
      "        `loss = mean(square(maximum(1 - y_true * y_pred, 0)), axis=-1)`\n",
      "        \n",
      "        Standalone usage:\n",
      "        \n",
      "        >>> y_true = np.random.choice([-1, 1], size=(2, 3))\n",
      "        >>> y_pred = np.random.random(size=(2, 3))\n",
      "        >>> loss = tf.keras.losses.squared_hinge(y_true, y_pred)\n",
      "        >>> assert loss.shape == (2,)\n",
      "        >>> assert np.array_equal(\n",
      "        ...     loss.numpy(),\n",
      "        ...     np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))\n",
      "        \n",
      "        Args:\n",
      "          y_true: The ground truth values. `y_true` values are expected to be -1 or\n",
      "            1. If binary (0 or 1) labels are provided we will convert them to -1 or\n",
      "            1. shape = `[batch_size, d0, .. dN]`.\n",
      "          y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n",
      "        \n",
      "        Returns:\n",
      "           Squared hinge loss values. shape = `[batch_size, d0, .. dN-1]`.\n",
      "\n",
      "DATA\n",
      "    LABEL_DTYPES_FOR_LOSSES = {<function sparse_softmax_cross_entropy>: 'i...\n",
      "    keras_export = functools.partial(<class 'tensorflow.python.util.tf_exp...\n",
      "\n",
      "FILE\n",
      "    /home/kmc3817/.conda/envs/keras-cpu/lib/python3.7/site-packages/keras/losses.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package keras.metrics in keras:\n",
      "\n",
      "NAME\n",
      "    keras.metrics - All Keras metrics.\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    base_metric\n",
      "    metrics\n",
      "\n",
      "FUNCTIONS\n",
      "    deserialize(config, custom_objects=None)\n",
      "        Deserializes a serialized metric class/function instance.\n",
      "        \n",
      "        Args:\n",
      "          config: Metric configuration.\n",
      "          custom_objects: Optional dictionary mapping names (strings) to custom\n",
      "            objects (classes and functions) to be considered during deserialization.\n",
      "        \n",
      "        Returns:\n",
      "            A Keras `Metric` instance or a metric function.\n",
      "    \n",
      "    get(identifier)\n",
      "        Retrieves a Keras metric as a `function`/`Metric` class instance.\n",
      "        \n",
      "        The `identifier` may be the string name of a metric function or class.\n",
      "        \n",
      "        >>> metric = tf.keras.metrics.get(\"categorical_crossentropy\")\n",
      "        >>> type(metric)\n",
      "        <class 'function'>\n",
      "        >>> metric = tf.keras.metrics.get(\"CategoricalCrossentropy\")\n",
      "        >>> type(metric)\n",
      "        <class '...metrics.CategoricalCrossentropy'>\n",
      "        \n",
      "        You can also specify `config` of the metric to this function by passing dict\n",
      "        containing `class_name` and `config` as an identifier. Also note that the\n",
      "        `class_name` must map to a `Metric` class\n",
      "        \n",
      "        >>> identifier = {\"class_name\": \"CategoricalCrossentropy\",\n",
      "        ...               \"config\": {\"from_logits\": True}}\n",
      "        >>> metric = tf.keras.metrics.get(identifier)\n",
      "        >>> type(metric)\n",
      "        <class '...metrics.CategoricalCrossentropy'>\n",
      "        \n",
      "        Args:\n",
      "          identifier: A metric identifier. One of None or string name of a metric\n",
      "            function/class or metric configuration dictionary or a metric function\n",
      "            or a metric class instance\n",
      "        \n",
      "        Returns:\n",
      "          A Keras metric as a `function`/ `Metric` class instance.\n",
      "        \n",
      "        Raises:\n",
      "          ValueError: If `identifier` cannot be interpreted.\n",
      "    \n",
      "    serialize(metric)\n",
      "        Serializes metric function or `Metric` instance.\n",
      "        \n",
      "        Args:\n",
      "          metric: A Keras `Metric` instance or a metric function.\n",
      "        \n",
      "        Returns:\n",
      "          Metric configuration dictionary.\n",
      "\n",
      "DATA\n",
      "    keras_export = functools.partial(<class 'tensorflow.python.util.tf_exp...\n",
      "\n",
      "FILE\n",
      "    /home/kmc3817/.conda/envs/keras-cpu/lib/python3.7/site-packages/keras/metrics/__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on data subset\n",
    "Validate the method by checking for instance when the model starts to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-cpu",
   "language": "python",
   "name": "keras-cpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
